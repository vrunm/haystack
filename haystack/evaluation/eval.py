from typing import Any, Callable, Dict, List, Optional, Tuple, Union

import numpy as np

from haystack import Pipeline, component
from haystack.evaluation.eval_utils import (
    convert_dict_to_objects,
    convert_objects_to_dict,
    flatten_list,
    get_grouped_values,
    group_values,
)
from haystack.evaluation.metrics import Metric, MetricsResult
from haystack.lazy_imports import LazyImport

with LazyImport(message="Run 'pip install transformers scikit-learn sentence-transformers'") as metrics_import:
    from sentence_transformers import CrossEncoder, SentenceTransformer
    from sklearn.metrics.pairwise import cosine_similarity
    from transformers import AutoConfig

import orjson


class EvaluationResult:
    """
    EvaluationResult keeps track of all the information related to evaluation, namely the runnable (Pipeline or component), inputs, outputs, and expected outputs.
    The EvaluationResult keeps track of all the information stored by eval.

    :param runnable: The runnable (Pipeline or component) used for evaluation.
    :param inputs: List of inputs used for evaluation.
    :param outputs: List of outputs generated by the runnable.
    :param expected_outputs: List of expected outputs used for evaluation.
    """

    def __init__(
        self,
        runnable: Union[Pipeline, component],
        inputs: List[Dict[str, Any]],
        outputs: List[Dict[str, Any]],
        expected_outputs: List[Dict[str, Any]],
    ) -> None:
        self.runnable = runnable
        self.inputs = inputs
        self.outputs = outputs
        self.expected_outputs = expected_outputs

        self.grouped_outputs = group_values(self.outputs)
        self.grouped_expected_outputs = group_values(self.expected_outputs)

        self.preds = get_grouped_values(self.grouped_outputs, "replies")
        self.labels = get_grouped_values(self.grouped_expected_outputs, "replies")

        if isinstance(self.preds, list) is isinstance(self.labels, list):
            self.preds = np.array(list(flatten_list(self.preds)))
            self.labels = np.array(list(flatten_list(self.labels)))
        assert isinstance(self.preds, np.ndarray) is isinstance(self.labels, np.ndarray)

    def serialize(self) -> bytes:
        """
        Serializes the contents of EvaluationResult, to bytes using orjson:
        - runnable
        - inputs
        - outputs
        - expected outputs

        The method uses theconvert_objects_to_dict() method to convert the outputs and expected_outputs to serialized data from dataclasses.

        :return bytes: The seriaized data in JSON.
        """
        type_runnable = "Pipeline" if isinstance(self.runnable, Pipeline) else "component"
        if type_runnable == "Pipeline":
            runnable_type_dict = ("Pipeline", self.runnable.dumps())
        else:
            runnable_type_dict = (type(self.runnable), self.runnable.to_dict())
        outputs_dict = convert_objects_to_dict(self.outputs)
        expected_outputs_dict = convert_objects_to_dict(self.expected_outputs)

        final_dict = {
            "runnable": runnable_type_dict,
            "inputs": self.inputs,
            "outputs": outputs_dict,
            "expected_outputs": expected_outputs_dict,
        }

        serialized_data = orjson.dumps(final_dict)

        return serialized_data

    @classmethod
    def deserialize(cls, data: bytes):
        """
        Deserializes the contents of EvaluationResult from JSON using orjson.

        The method uses the convert_dict_to_objects() method to convert the serialized outputs and expected_outputs to deserialized dataclasses.

        :param data: Data to be deserialized.
        :return: An instance of EvaluationResult representing the original object.
        """
        data = orjson.loads(data)

        runnable_type, runnable_dict = data["runnable"]
        if runnable_type == "Pipeline":
            runnable = Pipeline.loads(runnable_dict)
        else:
            runnable = runnable_type.from_dict()
        inputs = data["inputs"]
        outputs = convert_dict_to_objects(data=data["outputs"])
        expected_outputs = convert_dict_to_objects(data=data["expected_outputs"])

        return cls(runnable, inputs, outputs, expected_outputs)

    def calculate_metrics(self, metric: Union[Metric, Callable[..., MetricsResult]], **kwargs) -> MetricsResult:
        """
        Calculate evaluation metrics based on the provided Metric or using the custom metric function.

        :param metric: The Metric indicating the type of metric to calculate or custom function to compute.
        :return: MetricsResult containing the calculated metric.
        """
        if metric == Metric.RECALL:
            return self._calculate_recall(**kwargs)
        elif metric == Metric.ACCURACY:
            return self._calculate_accuracy(**kwargs)
        elif metric == Metric.F1:
            return self._calculate_f1(**kwargs)
        elif metric == Metric.EM:
            return self._calculate_em(**kwargs)
        elif metric == Metric.SAS:
            return self._calculate_sas(**kwargs)

        return metric(self, **kwargs)

    def _calculate_accuracy(self) -> MetricsResult:
        """
        Calculates the accuracy metric based on predicted and true labels.

        :param preds: Predicted values.
        :param labels: True labels.
        :return: MetricsResult: A dictionary containing the accuracy score.
        """
        labels = self.labels
        preds = self.preds

        correct = preds == labels
        return MetricsResult({"accuracy": correct.mean()})

    def _calculate_recall(self, pred_idx: int = 0) -> MetricsResult:
        """
        Recall measures how many times the correct Document was among the retrieved Documents over a set of queries.
        For a single query, the output is binary: either the correct Document is contained in the selection, or it is not.
        Over the entire dataset, the recall score amounts to a number between zero (no query retrieved the right document) and one (all queries retrieved the right Documents).

        Calculates the Recall metric based on predicted and true labels.

        :param preds: Predicted values.
        :param labels: True labels.
        :return: MetricsResult containing the Recall score.
        """
        preds = self.preds
        labels = self.labels

        label_start = labels[0]
        label_end = labels[-1]
        span = preds[pred_idx]
        pred_start = span.offset_answer_start
        pred_end = span.offset_answer_end

        if (pred_start + pred_end == 0) or (label_start + label_end == 0):
            if pred_start == label_start:
                return MetricsResult({"recall": 1.0})
            else:
                return MetricsResult({"recall": 0.0})
        pred_span = list(range(pred_start, pred_end + 1))
        label_span = list(range(label_start, label_end + 1))
        n_overlap = len([x for x in pred_span if x in label_span])
        if n_overlap == 0:
            return MetricsResult({"recall": 0.0})
        recall = n_overlap / len(label_span)
        return MetricsResult({"recall": recall})

    def _calculate_f1(self, pred_idx: int = 0) -> MetricsResult:
        """
        The F1 score measures the word overlap between the labeled and the predicted answer.

        Calculates the F1 metric based on predicted and true labels.

        :param preds: Predicted values.
        :param labels: True labels.
        :return: MetricsResult containing the F1 score.
        """
        preds = self.preds
        labels = self.labels

        label_start, label_end = labels
        span = preds[pred_idx]
        pred_start = span.offset_answer_start
        pred_end = span.offset_answer_end

        if (pred_start + pred_end == 0) or (label_start + label_end == 0):
            if pred_start == label_start:
                return MetricsResult({"f1": 1.0})
            else:
                return MetricsResult({"f1": 0.0})
        pred_span = list(range(pred_start, pred_end + 1))
        label_span = list(range(label_start, label_end + 1))
        n_overlap = len([x for x in pred_span if x in label_span])
        if n_overlap == 0:
            return MetricsResult({"f1": 0.0})
        precision = n_overlap / len(pred_span)
        recall = n_overlap / len(label_span)
        f1 = (2 * precision * recall) / (precision + recall)
        return MetricsResult({"f1": f1})

    def _calculate_em(self) -> MetricsResult:
        """
        Exact match measures the proportion of cases where the predicted Answer is identical to the correct Answer.

        Calculates the Exact Match metric based on predicted and true labels.

        :param preds: Predicted values.
        :param labels: True labels.
        :return: MetricsResult containing the Exact Match score.
        """
        preds = self.preds
        labels = self.labels

        n_docs = len(preds)
        n_correct = 0
        for pred, label in zip(preds, labels):
            qa_candidate = pred[0][0]
            pred_start = qa_candidate.offset_answer_start
            pred_end = qa_candidate.offset_answer_end
            curr_labels = label
            if (pred_start, pred_end) in curr_labels:
                n_correct += 1
        em = n_correct / n_docs if n_docs else 0
        return MetricsResult({"exact_match": em})

    def _calculate_sas(
        self,
        sas_model_name_or_path: str = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
        batch_size: int = 32,
        use_gpu: bool = True,
        use_auth_token: Optional[Union[str, bool]] = None,
    ) -> MetricsResult:
        """
        Computes Transformer-based similarity of predicted answer to gold labels to derive a more meaningful metric than EM or F1.
        Returns per QA pair a) the similarity of the most likely prediction (top 1) to all available gold labels
                            b) the highest similarity of all predictions to gold labels
                            c) a matrix consisting of the similarities of all the predictions compared to all gold labels

        :param predictions: Predicted answers as list of multiple preds per question
        :param labels: Labels as list of multiple possible answers per question
        :param sas_model_name_or_path: SentenceTransformers semantic textual similarity model, should be path or string
                                        pointing to downloadable models.
        :param batch_size: Number of prediction label pairs to encode at once.
        :param use_gpu: Whether to use a GPU or the CPU for calculating semantic answer similarity.
                        Falls back to CPU if no GPU is available.
        :param use_auth_token: The API token used to download private models from Huggingface.
                            If this parameter is set to `True`, then the token generated when running
                            `transformers-cli login` (stored in ~/.huggingface) will be used.
                            Additional information can be found here
                            https://huggingface.co/transformers/main_classes/model.html#transformers.PreTrainedModel.from_pretrained
        :return: MetricsResult containing the top_1_sas, top_k_sas, pred_label_matrix.
        """
        metrics_import.check()

        preds = self.preds
        labels = self.labels

        config = AutoConfig.from_pretrained(sas_model_name_or_path, use_auth_token=use_auth_token)
        cross_encoder_used = False
        if config.architectures is not None:
            cross_encoder_used = any(arch.endswith("ForSequenceClassification") for arch in config.architectures)

        device = None if use_gpu else "cpu"

        # Compute similarities
        top_1_sas = []
        top_k_sas = []
        pred_label_matrix = []
        lengths: List[Tuple[int, int]] = []

        # Based on Modelstring we can load either Bi-Encoders or Cross Encoders.
        # Similarity computation changes for both approaches
        if cross_encoder_used:
            model = CrossEncoder(
                sas_model_name_or_path,
                device=device,
                tokenizer_args={"use_auth_token": use_auth_token},
                automodel_args={"use_auth_token": use_auth_token},
            )
            grid = []
            for preds_, labels_ in zip(preds, labels):
                for p in preds_:
                    for l in labels_:
                        grid.append((p, l))
                lengths.append((len(preds_), len(labels_)))
            scores = model.predict(grid, batch_size=batch_size)

            current_position = 0
            for len_p, len_l in lengths:
                scores_window = scores[current_position : current_position + len_p * len_l]
                # Per predicted doc there are len_l entries comparing it to all len_l labels.
                # So to only consider the first doc we have to take the first len_l entries
                top_1_sas.append(np.max(scores_window[:len_l]))
                top_k_sas.append(np.max(scores_window))
                pred_label_matrix.append(scores_window.reshape(len_p, len_l).tolist())
                current_position += len_p * len_l
        else:
            # For Bi-encoders we can flatten predictions and labels into one list
            model = SentenceTransformer(sas_model_name_or_path, device=device, use_auth_token=use_auth_token)
            all_texts: List[str] = []
            for p, l in zip(preds, labels):  # type: ignore
                # TODO potentially exclude (near) exact matches from computations
                all_texts.extend(p)
                all_texts.extend(l)
                lengths.append((len(p), len(l)))
            # then compute embeddings
            embeddings = model.encode(all_texts, batch_size=batch_size)

            # then select which embeddings will be used for similarity computations
            current_position = 0
            for len_p, len_l in lengths:
                pred_embeddings = embeddings[current_position : current_position + len_p, :]
                current_position += len_p
                label_embeddings = embeddings[current_position : current_position + len_l, :]
                current_position += len_l
                sims = cosine_similarity(pred_embeddings, label_embeddings)
                top_1_sas.append(np.max(sims[0, :]))
                top_k_sas.append(np.max(sims))
                pred_label_matrix.append(sims.tolist())

        return MetricsResult(
            {"sas": {"top_1_sas": top_1_sas, "top_k_sas": top_k_sas, "pred_label_matrix": pred_label_matrix}}
        )


def eval(
    runnable: Union[Pipeline, component], inputs: List[Dict[str, Any]], expected_outputs: List[Dict[str, Any]]
) -> EvaluationResult:
    """
    Evaluates the provided Pipeline or component based on the given inputs and expected outputs.

    This function facilitates the evaluation of a given runnable (either a Pipeline or a component) using the provided
    inputs and corresponding expected outputs.

    :param runnable: The runnable (Pipeline or component) used for evaluation.
    :param inputs: List of inputs used for evaluation.
    :param expected_outputs: List of expected outputs used for evaluation.

    :return: An instance of EvaluationResult containing information about the evaluation, including the runnable, inputs, outputs, and expected outputs.
    """

    outputs = []

    # Check that expected outputs has the correct shape
    if len(inputs) != len(expected_outputs):
        raise ValueError("Length of expected_outputs does not match length of inputs.")

    for input_ in inputs:
        output = runnable.run(input_)
        outputs.append(output)

    return EvaluationResult(runnable, inputs, outputs, expected_outputs)
